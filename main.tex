%%
%% This is file `main.tex',
%% AgentCgroup Paper
%%

\documentclass[sigconf,nonacm]{acmart}

%% Disable microtype to avoid font expansion issues
\PassOptionsToPackage{disable}{microtype}

%% Disable ACM-specific requirements for draft
\settopmatter{printacmref=false,authorsperrow=3}
\renewcommand\footnotetextcopyrightpermission[1]{}

%% System name macro
\usepackage{xspace}
\newcommand{\sys}{\mbox{AgentCgroup}\xspace}

%% Graphics support
\usepackage{graphicx}

%% Allow slightly looser line breaking to avoid overfull hboxes in two-column
\emergencystretch=1.5em
\graphicspath{{docs/img/}}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{\sys: AI Agents Resource Control with eBPF}

% \author{Anonymous Author(s)}
% \affiliation{%
%   \institution{Anonymous Institution}
%   \city{Anonymous City}
%   \country{Anonymous Country}
% }
% \email{anonymous@example.com}

\author{Yusheng Zheng}
\affiliation{\institution{UC Santa Cruz}\country{}}
\email{yzhen165@ucsc.edu}

\author{Jiakun Fan}
\affiliation{\institution{Virginia Tech}\country{}}
\email{jiakunfan@vt.edu}

\author{Quanzhi Fu}
\affiliation{\institution{Virginia Tech}\country{}}
\email{quanzhif@vt.edu}

\author{Yiwei Yang}
\affiliation{\institution{UC Santa Cruz}\country{}}
\email{yyang363@ucsc.edu}

\author{Wei Zhang}
\affiliation{\institution{UConn}\country{}}
\email{wei.zhang@uconn.edu}

\author{Andi Quinn}
\affiliation{\institution{UC Santa Cruz}\country{}}
\email{aquinn1@ucsc.edu}


\begin{abstract}
As interactive AI agents are rapidly deployed in multi-tenant cloud environments, they dynamically execute diverse tool calls within sandboxed environments, each with distinct resource demands and rapid fluctuations. We present the first systematic characterization of OS-level resource dynamics in sandboxed AI coding agents, analyzing 144 SWE-rebench tasks across two models (cloud API and local GPU). Our measurements reveal that tool execution consumes approximately 40\% of agent active time, drives 98.5\% of memory bursts, and exhibits a 15.4$\times$ peak-to-average memory ratio, while container startup adds a further 29--45\% end-to-end latency overhead. Comparing these characteristics against serverless, microservice, and batch workloads, we identify three fundamental mismatches in existing resource management: a \emph{granularity mismatch} (container-level policies vs.\ tool-call-level dynamics), a \emph{responsiveness mismatch} (user-space reaction vs.\ sub-second unpredictable bursts), and an \emph{adaptability mismatch} (history-based prediction vs.\ non-deterministic stateful execution). We present \sys, an eBPF-based resource controller that addresses these mismatches through hierarchical cgroup structures aligned with tool-call boundaries, in-kernel enforcement via sched\_ext and memcg\_bpf\_ops, and runtime-adaptive policies. Preliminary evaluation demonstrates improved multi-tenant isolation and reduced resource waste.
\end{abstract}

% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011009.10011015</concept_id>
%        <concept_desc>Software and its engineering~Operating systems</concept_desc>
%        <concept_significance>500</concept_significance>
%    </concept>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011024</concept_id>
%        <concept_desc>Software and its engineering~Language features</concept_desc>
%        <concept_significance>300</concept_significance>
%    </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Software and its engineering~Operating systems}
% \ccsdesc[300]{Software and its engineering~Language features}

% \keywords{cgroup, eBPF, resource isolation, AI agents, operating systems}

\maketitle

\section{Introduction}

AI coding agents, systems that combine large language models with autonomous tool use to solve software engineering tasks, are rapidly moving from research prototypes to production deployments. Frameworks such as Claude Code~\cite{claude-code}, OpenHands~\cite{openhands}, and SWE-agent~\cite{swe-agent} execute iterative loops of LLM reasoning and tool calls (compilers, test runners, package managers) inside sandboxed containers. As cloud providers begin hosting dozens of concurrent agent instances on shared infrastructure, efficient resource management becomes critical. Yet we lack a systematic understanding of how these workloads actually consume resources.

To fill this gap, we conducted the first characterization of OS-level resource dynamics in sandboxed AI coding agents, analyzing 144 SWE-rebench~\cite{swe-rebench} tasks executed by two models, Haiku (cloud API) and GLM (local GPU), within the same agent framework. Our measurements reveal that LLM reasoning accounts for only 26--44\% of end-to-end task latency. Tool execution consumes approximately 40\% of agent active time and drives 98.5\% of all memory bursts, while container startup adds a further 29--45\% overhead. In other words, OS-level resource management directly affects more than half of the user-perceived task completion time.

Moreover, agent workloads differ fundamentally from traditional cloud workloads in ways that render existing resource controls ineffective. Our analysis reveals three fundamental mismatches:

\textbf{Granularity mismatch.} Resource demands vary at tool-call granularity (memory peak-to-average ratio reaches 15.4$\times$, and different Bash commands differ by 13.7$\times$ in memory consumption), but existing controls (cgroup limits, Kubernetes QoS) set a single static policy at container level. Static limits set to peak waste 91--93\% of allocated resources; limits set to average trigger OOM kills during 1--2 second tool bursts, destroying all accumulated LLM context.

\textbf{Responsiveness mismatch.} Memory bursts last only 1--2 seconds with change rates reaching several GB/s, and their timing is unpredictable due to non-deterministic tool-call sequences. User-space controllers (systemd-oomd, Kubernetes VPA) operate at tens-of-milliseconds to minutes timescales. By the time a user-space controller observes pressure and adjusts limits, the burst has already passed or caused interference.

\textbf{Adaptability mismatch.} Agent workloads are inherently non-deterministic: the same task produces 1.8$\times$ execution time variance across runs, and resource profiles change with the underlying model (1.7$\times$ CPU difference between Haiku and GLM). History-based prediction (moving averages, percentile recommendations, Autopilot~\cite{rzadca2020autopilot}) cannot reliably forecast resource needs, yet the traditional fallback of kill-and-restart is prohibitively expensive for in-process stateful agents.

We present \sys, an eBPF-based resource controller designed to close these three mismatches. \sys organizes resources using hierarchical cgroup v2 structures aligned with tool-call boundaries, executes control logic at kernel enforcement points via sched\_ext~\cite{sched-ext} and memcg\_bpf\_ops~\cite{memcg-bpf} for microsecond-level reaction, and supports runtime-adaptive policies driven by real-time observation rather than historical prediction.

\noindent Our contributions are:
\begin{itemize}
	\item \textbf{Characterization.} The first systematic measurement of OS-level resource dynamics in sandboxed AI coding agents (144 tasks, 2 models), revealing that tool execution dominates resource bursts and OS-level management affects over half of task latency (\S\ref{sec:characterization}).
	\item \textbf{Mismatch analysis.} A quantitative comparison with serverless, microservice, and batch workloads, identifying the granularity, responsiveness, and adaptability mismatches that render existing resource controls ineffective (\S\ref{sec:gap}).
	\item \textbf{System.} \sys, an eBPF-based resource controller with hierarchical cgroup domains, in-kernel enforcement, and runtime-adaptive policies, with preliminary evaluation demonstrating improved isolation and reduced waste (\S\ref{sec:design}, \S\ref{sec:eval}).
\end{itemize}

\section{Background}

\textbf{AI Coding Agents.} Modern AI coding agents combine large language models (LLMs) with tool-use capabilities to autonomously solve software engineering tasks. These agents operate in an iterative loop: the LLM reasons about the current state, selects a tool (e.g., file read, code edit, shell command, test execution), executes the tool in a sandboxed environment, observes the result, and repeats until the task is complete. Representative agents include Claude Code~\cite{claude-code}, OpenHands~\cite{openhands}, and SWE-agent~\cite{swe-agent}. Each tool invocation spawns distinct processes with varying resource profiles; for example, a compiler may consume gigabytes of memory, while a simple file read uses minimal resources. This creates highly dynamic, phase-varying workloads that challenge traditional resource management approaches. Recent work has begun analyzing agent costs from an inference infrastructure perspective~\cite{kim2025costdynamicreasoning} and proposing OS-level abstractions for LLM agents~\cite{mei2024aios,agentsight2025}, but none provides a systematic characterization of the OS-level resource dynamics inside agent sandboxes or designs kernel-level controls informed by such measurements.

\textbf{Linux cgroup} provides a hierarchical resource governance abstraction where the kernel organizes tasks into a tree of control groups and applies controller-specific accounting and enforcement along that hierarchy~\cite{cgroupv2}. The memory controller exposes two key boundaries: \texttt{memory\allowbreak.high} as a soft throttle point that triggers reclaim pressure without invoking the OOM killer, and \texttt{memory\allowbreak.max} as a hard limit that triggers OOM when exceeded. Cgroup~v2 also provides lifecycle controls: \texttt{cgroup\allowbreak.freeze} stops all processes in a subtree until unfrozen, \texttt{cgroup\allowbreak.kill} terminates all processes while handling concurrent forks, and \texttt{memory.oom\allowbreak.group} ensures atomic OOM termination to avoid partial failures.

\textbf{eBPF} enables Linux to address the tension between standardized interfaces and dynamic workloads by introducing programmable enforcement points, providing a safe and dynamically loadable mechanism for executing control logic inside the kernel~\cite{ebpf-verifier}. On the CPU side, \texttt{sched\_ext} allows scheduling policies to be defined by BPF programs with fail-safe reversion to default behavior on errors~\cite{sched-ext}. On the memory side, \texttt{memcg\_bpf\_ops} introduces hooks such as \texttt{get\_high\_delay\_ms} for custom throttle delays on \texttt{memory.high} breaches~\cite{memcg-bpf}. These primitives enable in-kernel enforcement with microsecond-level reaction times.

\section{Agent Workload Characterization}
\label{sec:characterization}

We conducted the first systematic measurement study of OS-level resource dynamics in sandboxed AI coding agents, focusing on three aspects: the execution model of agents, the dynamics of their resource usage, and the unpredictability of their resource demands.

\subsection{Experimental Setup}

All experiments run on a single machine (Intel Core Ultra 9 285K, 24 cores, 128\,GB DDR5, Ubuntu 24.04, Linux 6.15.11, cgroup v2). Each task executes in an isolated Podman container using official SWE-rebench~\cite{swe-rebench} images (2.9--17.3\,GB each) with no resource limits applied. We use the same agent framework (Claude Code) with two underlying models: (1)~Claude Haiku 4.5 (cloud API) and (2)~GLM-4.7-Flash (local GPU). We collect 111 tasks with GLM and 33 tasks with Haiku from SWE-rebench~\cite{swe-rebench}; the 33 Haiku tasks are a subset of the 111 GLM tasks, so all cross-model comparisons use this shared overlap. For each task, we sample CPU utilization and memory usage at 1-second intervals and record each tool call's type and timestamps. Tool execution time is measured from when the LLM emits a tool-use request to when the corresponding result is returned, encompassing framework dispatch, process execution, and result collection.

\subsection{Execution Model}

The execution process of agents differs fundamentally from traditional containerized workloads. Unlike serverless/FaaS processing short-lived stateless requests (100ms--2s)~\cite{shahrad2020serverless}, each agent task runs for approximately 10 minutes on average (GLM average 10.8 minutes, Haiku average 5.8 minutes, median 8.1 minutes; see Fig.~\ref{fig:exec}(a)) and executes stateful multi-round reasoning and tool call loops. Additionally, container startup has non-negligible fixed overhead, averaging 26.5 seconds (median 23.0 seconds), with a maximum of 97 seconds.

\textbf{LLM reasoning accounts for only 26--44\% of end-to-end task latency; the remainder is consumed by tool execution (${\sim}$40\% of active time) and container startup (29--45\%).} Agent execution consists of alternating LLM reasoning and tool call phases. As shown in Fig.~\ref{fig:exec}(b), across all 144 tasks (Haiku 33 + GLM 111), using \texttt{active\_time} (trace span excluding container startup) as denominator, tool execution time median is about 35\% (Haiku 34.7\%, GLM 36.5\%), with means of Haiku 42.5\% and GLM 36.4\%. The close medians suggest this $\sim$60/40 LLM-reasoning/tool-execution split is a common feature of agent workloads. However, this ratio varies dramatically between tasks, ranging from 0\% to 86\% (median ~36\%), as shown in Fig.~\ref{fig:tool_time}(a). Analyzing end-to-end task latency using \texttt{claude\_time} (full container lifecycle) as the denominator: container startup overhead accounts for 31--48\% (Haiku 47.7\%, GLM 31.0\%), tool execution accounts for approximately 26\% (consistent across both models: Haiku 25.9\%, GLM 25.5\%), and actual LLM reasoning accounts for only 26\% (Haiku) to 44\% (GLM). OS-level resource management (container startup and tool execution combined) thus directly affects 56--74\% of user-perceived task completion time.


\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{exec_overview}
	\caption{Task execution time distribution (a) and execution phase division (b).}
	\label{fig:exec}
\end{figure}

\textbf{Bash and Task dominate tool execution, spanning three orders of magnitude in duration.} As shown in Fig.~\ref{fig:tool_type}(a), Bash and Task (sub-agent calls) together account for over 90\% of total tool execution time in Haiku (47.8\% and 43.2\% respectively), while GLM relies almost entirely on Bash (99.5\% of tool time). Execution times span three orders of magnitude: Task sub-agents average ${\sim}$100\,s, Bash commands 4--6\,s, and lightweight tools such as Read and Edit under 0.5\,s (see Fig.~\ref{fig:tool_type}(a) for per-model detail). The two models adopt different strategies: Haiku distributes work via Task sub-agents and Web search, while GLM concentrates all computation in local Bash calls.

Further analyzing Bash command semantics (Fig.~\ref{fig:tool_type}(b)), test execution (pytest, unittest, etc.) dominates Bash time in both models (Haiku 72.9\%, GLM 43.7\%), followed by package installation (${\sim}$10\%) and Python snippets (GLM 26.9\%). Resource demands vary accordingly: test execution is CPU and memory intensive, while file exploration and Git operations are lightweight. Tool calls also follow a temporal ``understand-modify-verify'' pattern (Fig.~\ref{fig:tool_time}(b)): Read concentrates in the first 30\% of execution, Bash peaks in 40--80\%, and Edit distributes evenly.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{tool_bash_breakdown}
	\caption{Tool execution time distribution (a) and Bash command semantic category proportion (b), GLM agent.}
	\label{fig:tool_type}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{tool_time_pattern}
	\caption{Tool time proportion distribution (a) and tool call distribution over execution progress (b), all 144 tasks.}
	\label{fig:tool_time}
\end{figure}

\subsection{Resource Dynamics}

Building on the execution model above, we now examine which resource dimensions tool calls stress most, how resource consumption fluctuates over time, and how CPU and memory dynamics interact.

\textbf{Memory, not CPU, is the primary bottleneck for multi-tenant concurrency density.} Agent average CPU utilization is low (Haiku 13.2\%, GLM 7.6\%, single-core baseline), well below saturation on the 24-core experimental platform. Peak memory can reach 2--4\,GB, so 128\,GB of RAM with peak allocation supports only 32--64 instances, while CPU utilization at this concurrency remains below 36\% of total capacity.

\textbf{Resource consumption exhibits a two-layer structure: a ${\sim}$185\,MB framework baseline plus tool-call bursts.} As shown in Fig.~\ref{fig:resource}(b), the agent framework (e.g., Claude Code's Node.js runtime) maintains an irreducible memory baseline: across all 144 tasks, average memory in early execution (first 10\% of sampling points) is Haiku 183\,MB, GLM 188\,MB, median about 185\,MB. Even during LLM reasoning phases (no tool calls), memory remains at this baseline level. Resource fluctuations come almost entirely from subprocesses spawned by tool calls: test execution, dependency installation, and other operations raise memory to 500\,MB--2\,GB, then fall back to baseline. After normalizing memory usage of all tasks to execution progress and aggregating (Fig.~\ref{fig:resource}(b)), the first half of execution (0--50\%) maintains a relatively stable baseline (about 185--200\,MB), while the second half shows an upward trend with greater variance, consistent with resource consumption during Bash-dense phases. In multi-tenant scenarios, 64 concurrent instances require about 12\,GB of memory for framework baseline alone, on top of which tool bursts add an order-of-magnitude higher peak demand.

\textbf{Within the burst layer, resource consumption is determined by tool semantics rather than tool type: Bash calls across different semantic categories differ by 13.7$\times$ in peak memory.} For example, among 18 representative tasks, Bash calls in pydicom/pydicom\#2022 (a medical imaging library) consume average peak memory of 4\,GB, while streamlink/streamlink\#2160 (a network streaming tool) needs only 291\,MB. Across Bash command categories, test execution (pytest, etc.) P95 memory spike reaches 518\,MB (Haiku)/234\,MB (GLM) with average CPU spike +3.2\%; package installation P95 spike is about 233\,MB; file exploration and Git operations average only 4.5\,MB and 13.5\,MB respectively.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{resource_profile}
	\caption{Docker image size distribution (a) and aggregated memory trajectory (b), all 144 tasks.}
	\label{fig:resource}
\end{figure}

\textbf{Resource usage follows a burst-silence pattern, with 98.5\% of memory bursts occurring during tool calls.} Agent workloads exhibit dramatic temporal fluctuation: memory changes by up to 2.9\,GB within a single 1-second interval and CPU peaks exceed 100\% (multi-core). Fig.~\ref{fig:timeseries_haiku} and Fig.~\ref{fig:timeseries_glm} illustrate this with two representative tasks, each showing CPU (blue, top) and memory (green, bottom) with tool-call intervals shaded red. Resource spikes align closely with tool calls, while LLM reasoning phases show stable, low usage. Haiku tool calls produce concentrated CPU multi-core spikes (peaks exceeding 175\%), while GLM tool calls are dense and frequent but CPU stays low ($\sim$10\%), with fluctuations mainly in memory. Despite per-task variation, this tool-driven burst-silence pattern is universal across both datasets. Quantitatively, labeling each sampling point as ``during tool call'' or ``during LLM reasoning'' and counting memory bursts exceeding 300\,MB: in Haiku, tool calls occupy only 50.6\% of sampling time yet contain 98.5\% of memory bursts; in GLM, 35.9\% of time contains 67.3\% of bursts (1.9$\times$ concentration). CPU burst attribution is more dispersed (Haiku 55.3\%, GLM 30.2\%), reflecting GLM's local GPU inference overhead. This asymmetry, memory bursts concentrated in tool calls while CPU bursts are dispersed, is consistent across both datasets.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{rq1_resource_timeseries}
	\caption{Resource usage time series: Haiku agent executing pre-commit/pre-commit\#2524.}
	\label{fig:timeseries_haiku}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{rq1_resource_timeseries_qwen}
	\caption{Resource usage time series: GLM agent executing sigmavirus24/github3.py\#673.}
	\label{fig:timeseries_glm}
\end{figure}

\textbf{Resource bursts last 1--2 seconds with peak-to-average ratio up to 15.4$\times$, an order of magnitude beyond traditional cloud workloads.} Among 18 representative tasks, pydicom/pydicom\#2022 has peak memory of 4060\,MB while average memory is only 264\,MB, a 15.4$\times$ ratio; this peak falls back to the 230\,MB baseline within seconds (Fig.~\ref{fig:timeseries_haiku}, Fig.~\ref{fig:timeseries_glm}). Memory peaks concentrate in the latter half of execution (mean/median $\sim$65\% progress) but occur throughout the execution cycle. Change rates are equally extreme: maximum memory change rate reaches 3\,GB/s, CPU change rate exceeds 50\%/second, with significant change events accounting for 1.7\%--3.8\% of all sampling points (Fig.~\ref{fig:change_rate}).

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{rq1_change_rate_distribution}
	\caption{Resource change rate distribution (CPU and memory), Haiku dataset.}
	\label{fig:change_rate}
\end{figure}

\textbf{85\%--97\% of tasks contain retry loops with progressive memory accumulation.} Retry is inherent behavior of agent workloads: 85\% (28/33) of tasks in Haiku dataset contain retry groups (3 or more consecutive Bash calls), in GLM dataset this ratio reaches 97\% (108/111). GLM averages 3.9 retry groups per task, maximum consecutive retries up to 56 times, retry time averages 20.5\% of total execution time (Haiku 7.4\%). This ``execute test, observe failure, modify code, test again'' iterative pattern is agent's behavioral signature. More importantly, each retry cycle retains previous memory context without cleanup, causing progressive memory accumulation; in the most extreme case, retries cause 502\,MB of memory accumulation that is not released. This pattern is common in agents but rare in traditional workloads, and its resource impact is unpredictable.

\textbf{CPU-memory correlation varies by task ($-$0.84 to $+$0.50); co-directional change cannot be assumed.} The average correlation coefficient across all tasks is $-$0.39: some tasks show positive correlation (tool execution pulls up both CPU and memory simultaneously), while others show negative correlation (CPU-intensive phases coincide with lower memory demand).

\textbf{Agent container images average 3.5\,GB, 7$\times$ larger than typical microservice images and 70$\times$ larger than serverless functions.} As shown in Fig.~\ref{fig:resource}(a), Docker image sizes across 114 deduplicated images range from 2.9 to 17.3\,GB (mean 4.1, median 3.5, concentrated in 3--4\,GB), with GLM's 111 tasks totaling 456\,GB. This large image footprint also increases the cost of container restarts: pulling or re-initializing a 3.5\,GB image adds substantial overhead to any kill-and-restart recovery strategy.

\subsection{Unpredictability}

\textbf{Preliminary evidence suggests high non-determinism: 1.8$\times$ execution time variance across runs of the same task.} Unlike traditional containerized workloads, agent workloads exhibit high non-determinism. As a case study, we executed the same task (DevOps\_Build\_Hard) three times, observing execution times of 402, 222, and 259 seconds respectively, a 1.8$\times$ difference. The three runs produced entirely different solutions: different code modifications, file counts, and strategies. This non-determinism stems from LLM reasoning randomness and decision-path diversity. Even LLM-observable proxies offer no help: conversation rounds correlate moderately with execution time (r\,=\,+0.57 to +0.82) but not with peak memory (r\,$<$\,0.11), confirming that resource consumption is driven by tool-call semantics rather than reasoning scale.

\textbf{Resource demands vary 20$\times$ across tasks and diverge further across models.} Among 18 representative tasks, peak memory requirements range from 197\,MB to 4\,GB (CV\,=\,147\%): scientific computing tasks (numba/numba\#5721, pydicom/pydicom\#2022) exhibit significantly higher memory than CLI tools (joke2k/faker\#1520) or network utilities (streamlink/streamlink\#2160), yet all run in the same container. Model choice amplifies this variation: Haiku and GLM show 1.7$\times$ CPU utilization difference on same tasks, with execution times also differing significantly (Haiku 352 seconds, GLM 664 seconds). Even with identical agent framework (both Claude Code), the underlying model alone determines the resource profile. Haiku performs inference via cloud API, where API calls, response parsing, and context management consume more local CPU; GLM's inference executes on local GPU, so CPU load comes almost entirely from tool calls (GLM: only 0.5\% of sampling points exceed 50\% CPU, vs.\ Haiku 8.2\%). Even on identical tasks, CPU distribution, duration, and memory trajectories differ between models, precluding a one-size-fits-all ``agent workload'' resource profile.

\section{Resource Management Mismatches}
\label{sec:gap}

The characterization in \S\ref{sec:characterization} reveals that agent workloads differ from known cloud workloads along several dimensions (Table~\ref{tab:comparison}). These differences create three resource management mismatches (Table~\ref{tab:mismatch}): granularity, responsiveness, and adaptability. We analyze each mismatch below, explaining why solutions from kernel cgroup interfaces to cluster-level autoscaling cannot bridge them.

\begin{table*}[t]
	\centering
	\begin{tabular}{lcccc}
		\hline
		\textbf{Dimension} & \textbf{Serverless/FaaS}~\cite{shahrad2020serverless} & \textbf{Microservices}~\cite{cortez2017resource} & \textbf{Batch/HPC}~\cite{verma2015borg} & \textbf{AI Coding Agent}                    \\
		\hline
		Execution duration & 100ms--2s                & Long-running           & Minutes--hours     & \textbf{5--11 minutes}                      \\
		Container image    & $\sim$50\,MB             & 100\,MB--1\,GB         & 1--10\,GB          & \textbf{2.9--17.3\,GB (med.\ 3.5)}          \\
		Statefulness       & Stateless                & External state         & Stateful           & \textbf{In-process stateful}                \\
		Memory footprint   & 128--512\,MB             & Steady $\sim$1\,GB     & Scales with data   & \textbf{185\,MB idle, peaks 2--4\,GB}       \\
		Memory peak/avg    & $\sim$1.5$\times$        & 2--3$\times$           & $\sim$1$\times$    & \textbf{15.4$\times$}                       \\
		CPU utilization    & Brief spike              & 10--40\%               & 80--100\%          & \textbf{$<$13\% avg, peaks $>$175\%}        \\
		Determinism        & Deterministic            & Mostly deterministic   & Deterministic      & \textbf{1.8$\times$ variance for same task} \\
		Resource pattern   & Flat                     & Steady + daily cycle   & Stable rise        & \textbf{Burst-silence alternating}          \\
		Termination cost   & Just retry               & Can migrate            & Lose progress      & \textbf{Lose all LLM context}               \\
		\hline
	\end{tabular}
	\caption{Quantitative comparison of AI coding agent workloads with typical cloud workloads.}
	\label{tab:comparison}
\end{table*}

\begin{table}[t]
	\centering
	\small
	\setlength{\tabcolsep}{3pt}
	\begin{tabular}{p{1.2cm}p{1.9cm}p{1.9cm}p{1.9cm}}
		\hline
		& \textbf{Static Limits} & \textbf{Reactive Control} & \textbf{Predictive Scaling} \\
		\hline
		\textbf{Tools} & mem.max/high, cpu.max~\cite{cgroupv2}; K8s QoS~\cite{k8s-qos} & PSI~\cite{psi}; oomd~\cite{systemd-oomd,meta-oomd}; TMO~\cite{tmo} & VPA~\cite{k8s-vpa}; Autopilot~\cite{rzadca2020autopilot} \\
		\textbf{Assumes} & Known peak; stable demand & Gradual pressure; kill acceptable & Repeatable; history valid \\
		\textbf{Agent} & 15.4$\times$ peak/avg; tool-semantic variation & 1--2\,s burst; unpredictable timing & 1.8$\times$ variance; kill\,=\,lose context \\
		\textbf{Mismatch} & Granularity & Responsiveness & Adaptability \\
		\hline
	\end{tabular}
	\caption{Existing resource management tools and their mismatches with agent workloads.}
	\label{tab:mismatch}
\end{table}

\subsection{Granularity Mismatch}

Agent resource demands vary at tool-call granularity, but all existing controls set a single policy at container level. The memory peak-to-average ratio illustrates this most clearly: Azure Functions exhibit near-flat memory~\cite{shahrad2020serverless}, Azure VMs stay within 2--3$\times$~\cite{cortez2017resource}, and Google Autopilot recommends within 2$\times$ of actual peaks~\cite{rzadca2020autopilot}, whereas agent workloads reach 15.4$\times$.

Cgroup~v2 hard limits (\texttt{memory\allowbreak.max}) trigger OOM kill synchronously in the page allocation path. For traditional workloads with peak/avg ratio 2--3$\times$, setting limits to peak causes 50--67\% overprovisioning, usually acceptable. But with this ratio, setting to peak (4060\,MB) wastes 93\% of allocated memory (and the peak-level allocation is unnecessary 98\% of the time), while setting to average triggers OOM kills during 1--2 second tool bursts, destroying all accumulated agent state. Across all 144 tasks, CPU overprovisioning by peak allocation reaches 11.1$\times$--13.9$\times$. A static threshold fundamentally cannot express the ``low baseline + high burst'' two-layer structure; moreover, retry loops (present in 85--97\% of tasks, \S\ref{sec:characterization}) cause progressive memory accumulation across iterations, so a limit that suffices for the first retry may trigger OOM by the fifth. Soft limits (\texttt{memory\allowbreak.high}) avoid OOM kills through reclaim and throttling, which is effective for traditional workloads where reclaim targets file page cache without affecting correctness. However, agent workloads have two structural problems: reclaim cannot distinguish framework memory (Node.js heap, V8 JIT cache, ${\sim}$185\,MB) from tool subprocess memory, and acting on framework pages causes V8 GC pressure and JIT cache invalidation that extends LLM response parsing time; additionally, a single threshold at container granularity cannot differentiate \texttt{git~status} (13.5\,MB average spike) from \texttt{pytest} (518\,MB P95 spike). Kubernetes QoS classes~\cite{k8s-qos} face the same limitation: Guaranteed incurs comparable waste, BestEffort is unsuitable for stateful agents, and Burstable with CFS bandwidth control (\texttt{cpu\allowbreak.max}) still operates at container granularity, unable to distinguish the LLM reasoning phase (near-zero CPU, low memory) from tool execution (multi-core burst, high memory) or set per-tool-call quotas. Agent CPU demand is bimodal (near-idle most of the time, yet tool execution peaks exceed 175\%), and a single bandwidth quota cannot adapt to both modes.

The granularity mismatch extends across resource dimensions. CPU--memory correlation varies from $-$0.84 to $+$0.50 across tasks (\S\ref{sec:characterization}): memory bursts concentrate in tool calls (98.5\%) while CPU bursts are more dispersed (55.3\%), so the two resources require different management granularities. Kubernetes QoS classes tie CPU and memory policies together (Guaranteed requires both limits; Burstable applies the same class to both), and VPA derives recommendations from the same historical window. But with task-dependent and even anti-correlated resource dimensions, a single policy granularity, whether per-container or per-tool-call, cannot express the asymmetry. Furthermore, the large image footprint (median 3.5\,GB, \S\ref{sec:characterization}) makes container creation and destruction heavyweight operations (startup averaging 26.5 seconds), precluding strategies that achieve finer granularity by spawning additional containers and reinforcing the need for sub-container resource domains within the existing container lifecycle.

\subsection{Responsiveness Mismatch}

Agent resource bursts last only 1--2 seconds with change rates reaching several GB/s, a burst-silence pattern unlike the flat profiles of serverless functions, the diurnal cycles of microservices, or the steady ramps of batch jobs (Table~\ref{tab:comparison}). Moreover, their timing is unpredictable due to non-deterministic tool-call sequences. This combination of fast and unpredictable bursts means controllers must react in real time at kernel speed, since pre-allocation based on prediction is not viable. User-space controllers cannot meet this requirement. PSI-driven solutions (systemd-oomd~\cite{systemd-oomd}, Meta oomd~\cite{meta-oomd}) monitor cgroup Pressure Stall Information and take action (kill the cgroup or trigger swap) when pressure exceeds a threshold. Their design assumes (a) a sufficient time window for decision-making after the pressure signal appears, and (b) that kill or migration is an acceptable degradation path. Both assumptions fail for agents: the full cycle from PSI signal generation to user-space daemon reception, decision, and cgroup control file write takes tens of milliseconds, by which time the burst has already passed or triggered kernel intervention. PSI is also a container-level aggregate that cannot attribute pressure to specific tool calls; it reports ``container has pressure'' but not ``pytest caused it.'' Kubernetes VPA~\cite{k8s-vpa} adjusts resources at Pod restart level (stable) or minute-level in-place resize~\cite{k8s-inplace-resize} (alpha feature), orders of magnitude slower than second-scale tool bursts, making within-execution adjustment impossible. When slow reaction does lead to OOM, recovery is itself slow: restarting a 3.5\,GB agent container takes 26--97 seconds (\S\ref{sec:characterization}), two to three orders of magnitude longer than serverless cold starts, amplifying the cost of every missed burst.

\subsection{Adaptability Mismatch}

Traditional cloud workloads are largely deterministic, allowing history-based resource management: Borg's~\cite{verma2015borg} utilization data guides allocation, and Autopilot~\cite{rzadca2020autopilot} uses historical P95 metrics for automatic recommendations. Agent workloads violate this assumption along three axes. First, resource demand varies 20$\times$ across tasks (joke2k/faker\#1520 peak memory 197\,MB vs.\ pydicom/pydicom\#2022 4060\,MB), so VPA~\cite{k8s-vpa}-recommended values are inevitably too high or too low. Second, even the same task produces different resource demands across runs (1.8$\times$ execution time variance), and output token count has near-zero correlation with peak memory (r\,=\,$-$0.14), making historical percentiles statistically meaningless. Third, within a single execution, retry loops (85--97\% of tasks, \S\ref{sec:characterization}) cause progressive memory accumulation (up to 502\,MB unreleased in the most extreme case), so memory demands grow unpredictably even during one run, not only across runs.

This unpredictability is compounded by the prohibitive cost of the traditional fallback: kill-and-restart imposes a \emph{triple penalty} on agent workloads. First, \emph{slow recovery}: agent container images average 3.5\,GB (median), so cold-start recovery (pulling layers, re-initializing the environment) takes 26--97 seconds (\S\ref{sec:characterization}), two to three orders of magnitude longer than serverless cold starts (${\sim}$50\,MB images). Second, \emph{lost state}: an OOM kill destroys an average of 10 minutes of accumulated in-process LLM context, and unlike microservices with external state stores, this context cannot be checkpointed or migrated. Third, \emph{non-deterministic re-execution}: re-running the same task follows an entirely different solution path (1.8$\times$ time variance, \S\ref{sec:characterization}), so restart does not even guarantee convergence to the original solution. Any strategy that relies on termination as fallback, including Kubernetes eviction, systemd-oomd~\cite{systemd-oomd}, and static OOM kill, incurs all three penalties simultaneously. Effective resource management must therefore adapt at runtime based on real-time observation, with graceful degradation (throttling, freezing) rather than termination.

In summary, control granularity is mismatched with tool-call-level dynamics, responsiveness is mismatched with sub-second unpredictable bursts, and adaptability is mismatched with non-deterministic stateful execution. \sys is designed to close these three mismatches.

\section{\sys Design and Implementation}
\label{sec:design}

\sys closes the three mismatches identified in \S\ref{sec:gap}: fine-grained resource domains match tool-call-level dynamics, in-kernel eBPF enforcement reacts at microsecond timescales, and runtime-adaptive policies replace historical prediction with graceful degradation.

\textbf{Fine-grained resource domains.} The granularity mismatch (\S\ref{sec:gap}) arises because existing controls set a single policy per container, while agent demands vary per tool call. \sys organizes resources using a hierarchical cgroup v2 structure where each agent workload maps to a cgroup node with tool calls as child nodes, enabling per-tool-call resource constraints while maintaining overall workload budgets. For recovery, \sys uses cgroup v2 lifecycle primitives: freezing subtrees when tool calls exceed soft limits, and atomically killing subtrees when termination is necessary.

\textbf{In-kernel enforcement.} The responsiveness mismatch (\S\ref{sec:gap}) arises because user-space controllers react at millisecond-to-minute timescales, while agent bursts last 1--2 seconds with unpredictable timing. \sys executes control logic directly at kernel cgroup enforcement points via eBPF with the toolcall cgroup, enabling microsecond-level reaction without user-kernel round trips. On CPU, \texttt{sched\_ext}~\cite{sched-ext} maintains per-workload and per-tool-call metadata in BPF maps, prioritizing latency-sensitive tool calls with automatic fail-safe reversion on errors. On memory, \texttt{memcg\_bpf\_ops} hooks~\cite{memcg-bpf} implement custom throttling delays when a cgroup breaches its soft limit (\texttt{memory.high}), with \texttt{memory.max} as the hard limit~\cite{cgroupv2}.

\textbf{Runtime-adaptive policies with graceful degradation.} The adaptability mismatch (\S\ref{sec:gap}) arises because history-based prediction fails for non-deterministic agent workloads, and kill-restart destroys accumulated LLM context. \sys uses eBPF to trace process creation and memory allocation changes in-kernel, detecting tool-call boundaries and resource dynamics in real time without user-space polling. When memory pressure rises, the BPF program applies graduated responses (throttling via \texttt{memory.high} delays, freezing via \texttt{cgroup.freeze}) rather than termination, preserving agent state. A lightweight user-space daemon handles cgroup lifecycle management and policy configuration via shared BPF maps. \sys is implemented in C using libbpf and BPF CO-RE~\cite{libbpf} for portability, with eBPF verification~\cite{ebpf-verifier} ensuring safety. Our prototype runs on Linux 6.15 with memcg\_bpf\_ops patches~\cite{memcg-bpf} (currently under upstream review).

\section{Preliminary Evaluation}
\label{sec:eval}

We evaluate \sys by replaying real agent memory traces from \S\ref{sec:characterization} at accelerated speed in a multi-tenant setting with memcg BPF struct\_ops enforcement. No application code is modified; isolation is achieved entirely through cgroup boundaries and BPF hooks.

\textbf{Setup.} Three agent traces run concurrently in separate cgroups: dask/dask\#11628 as the HIGH-priority session (peak 421\,MB) and two instances of sigmavirus24/github3.py\#673 as LOW-priority sessions (peak 406\,MB each). Under BPF, LOW cgroups are throttled via \texttt{get\_high\_delay\_ms} when the HIGH cgroup experiences memory pressure; the HIGH cgroup is protected via \texttt{below\_low}. We compare against a no-isolation baseline under two memory pressure scenarios (Fig.~\ref{fig:eval}).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{eval_results}
\caption{BPF enforcement evaluation with real agent trace replay. (a)~OOM survival rate under tight memory (1100\,MB total for ${\sim}$1233\,MB demand). (b)~HIGH-priority P95 allocation latency under moderate memory (1300\,MB).}
\label{fig:eval}
\end{figure}

\textbf{Results.} Under tight memory (1100\,MB total for ${\sim}$1233\,MB combined demand), the baseline OOM-kills one LOW process (66\% survival); BPF allows all processes to complete (100\%) by throttling LOW allocations (239 delay triggers) while HIGH finishes with only +2.8\% overhead (Fig.~\ref{fig:eval}(a)). BPF also reduces HIGH-priority P95 allocation latency by 29\% (70.97$\to$50.14\,ms) through reduced memory contention (Fig.~\ref{fig:eval}(b)). Enforcement overhead is negligible: P50 latency increases by 0.3\% and total completion time decreases by 1.1\%. Kernel selftests confirm that BPF throttling delays match the configured value within 2.3\% relative error (2000\,ms configured vs.\ 2.000\,$\pm$\,0.046\,s measured).

\section{Conclusion}

We presented \sys, an eBPF-based resource controller for interactive AI agent workloads. By executing control logic at kernel enforcement points via sched\_ext and memcg\_bpf\_ops, \sys addresses the granularity mismatch through tool-call-aligned cgroup domains, the responsiveness mismatch through microsecond-level in-kernel reaction, and the adaptability mismatch through runtime-adaptive policies with graceful degradation. In-kernel enforcement via eBPF is necessary for effective resource management of agent workloads whose sub-second, non-deterministic, stateful bursts invalidate the assumptions of existing resource controllers.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
