%%
%% This is file `main.tex',
%% AgentCgroup Paper
%%

\documentclass[sigconf,nonacm]{acmart}

%% Disable microtype to avoid font expansion issues
\PassOptionsToPackage{disable}{microtype}

%% Disable ACM-specific requirements for draft
\settopmatter{printacmref=false,authorsperrow=3}
\renewcommand\footnotetextcopyrightpermission[1]{}

%% System name macro
\usepackage{xspace}
\newcommand{\sys}{\mbox{AgentCgroup}\xspace}

%% Graphics support
\usepackage{graphicx}

%% Allow slightly looser line breaking to avoid overfull hboxes in two-column
\emergencystretch=1.5em
\graphicspath{{docs/img/}}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{\sys: AI Agents Resource Control with eBPF}

% \author{Anonymous Author(s)}
% \affiliation{%
%   \institution{Anonymous Institution}
%   \city{Anonymous City}
%   \country{Anonymous Country}
% }
% \email{anonymous@example.com}

\author{Yusheng Zheng}
\affiliation{\institution{UC Santa Cruz}\country{}}
\email{yzhen165@ucsc.edu}

\author{Jiakun Fan}
\affiliation{\institution{Virginia Tech}\country{}}
\email{jiakunfan@vt.edu}

\author{Quanzhi Fu}
\affiliation{\institution{Virginia Tech}\country{}}
\email{quanzhif@vt.edu}

\author{Yiwei Yang}
\affiliation{\institution{UC Santa Cruz}\country{}}
\email{yyang363@ucsc.edu}

\author{Wei Zhang}
\affiliation{\institution{UConn}\country{}}
\email{wei.zhang@uconn.edu}

\author{Andi Quinn}
\affiliation{\institution{UC Santa Cruz}\country{}}
\email{aquinn1@ucsc.edu}


\begin{abstract}
Rapidly being deployed in multi-tenant cloud environments, Interactive AI agents dynamically execute diverse tool calls within sandboxed environments, each with distinct resource demands and rapid fluctuations. We present the first systematic characterization of resource usage in production AI coding agents, analyzing 144 SWE-rebench tasks across two models (cloud API and local GPU). Our measurements reveal that tool execution consumes approximately 40\% of agent active time, drives 98.5\% of memory bursts, and exhibits a 15.4$\times$ peak-to-average memory ratio, while container startup adds a further 29--45\% end-to-end latency overhead. Comparing these characteristics against serverless, microservice, and batch workloads, we identify three fundamental gaps in existing resource management: a \emph{granularity gap} (container-level policies vs.\ tool-call-level dynamics), a \emph{timescale gap} (user-space reaction vs.\ second-scale bursts), and a \emph{predictability gap} (history-based prediction vs.\ non-deterministic execution). We present \sys, an eBPF-based resource controller that addresses these gaps through hierarchical cgroup structures aligned with tool-call boundaries, in-kernel enforcement via sched\_ext and memcg\_bpf\_ops, and runtime-adaptive policies. Preliminary evaluation demonstrates improved multi-tenant isolation and reduced resource waste.
\end{abstract}

% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011009.10011015</concept_id>
%        <concept_desc>Software and its engineering~Operating systems</concept_desc>
%        <concept_significance>500</concept_significance>
%    </concept>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011024</concept_id>
%        <concept_desc>Software and its engineering~Language features</concept_desc>
%        <concept_significance>300</concept_significance>
%    </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Software and its engineering~Operating systems}
% \ccsdesc[300]{Software and its engineering~Language features}

% \keywords{cgroup, eBPF, resource isolation, AI agents, operating systems}

\maketitle

\section{Introduction}

AI coding agents, systems that combine large language models with autonomous tool use to solve software engineering tasks, are rapidly moving from research prototypes to production deployments. Frameworks such as Claude Code~\cite{claude-code}, OpenHands~\cite{openhands}, and SWE-agent~\cite{swe-agent} execute iterative loops of LLM reasoning and tool calls (compilers, test runners, package managers) inside sandboxed containers. As cloud providers begin hosting dozens of concurrent agent instances on shared infrastructure, efficient resource management becomes critical. Yet we lack a systematic understanding of how these workloads actually consume resources.

To fill this gap, we conducted the first characterization of resource usage in production AI coding agents, analyzing 144 SWE-rebench~\cite{swe-rebench} tasks executed by two models, Haiku (cloud API) and GLM (local GPU), within the same agent framework. Our measurements reveal that LLM reasoning accounts for only 26--44\% of end-to-end task latency. Tool execution consumes approximately 40\% of agent active time and drives 98.5\% of all memory bursts, while container startup adds a further 29--45\% overhead. In other words, OS-level resource management directly affects more than half of the user-perceived task completion time.

Moreover, agent workloads differ fundamentally from traditional cloud workloads in ways that render existing resource controls ineffective. Our analysis reveals three gaps:

\textbf{Granularity gap.} Resource demands vary at tool-call granularity (memory peak-to-average ratio reaches 15.4$\times$, and different Bash commands differ by 13.7$\times$ in memory consumption), but existing controls (cgroup limits, Kubernetes QoS) set a single static policy at container level. Static limits set to peak waste 91--93\% of allocated resources; limits set to average trigger OOM kills during 1--2 second tool bursts, destroying all accumulated LLM context.

\textbf{Timescale gap.} Memory bursts last only 1--2 seconds with change rates reaching several GB/s, yet user-space controllers (systemd-oomd, Kubernetes VPA) operate at tens-of-milliseconds to minutes timescales. By the time a user-space controller observes pressure and adjusts limits, the burst has already passed or caused interference.

\textbf{Predictability gap.} Agent workloads are inherently non-deterministic: the same task produces 1.8$\times$ execution time variance across runs, and resource profiles change with the underlying model (1.7$\times$ CPU difference between Haiku and GLM). History-based prediction (moving averages, percentile recommendations, Autopilot~\cite{rzadca2020autopilot}) cannot reliably forecast resource needs.

We present \sys, an eBPF-based resource controller designed to close these three gaps. \sys organizes resources using hierarchical cgroup v2 structures aligned with tool-call boundaries, executes control logic at kernel enforcement points via sched\_ext~\cite{sched-ext} and memcg\_bpf\_ops~\cite{memcg-bpf} for microsecond-level reaction, and supports runtime-adaptive policies driven by real-time observation rather than historical prediction.

\noindent Our contributions are:
\begin{itemize}
	\item \textbf{Characterization.} The first systematic measurement of resource usage in production AI coding agents (144 tasks, 2 models), revealing that tool execution drives 98.5\% of memory bursts and OS-level management affects over half of task latency (\S\ref{sec:characterization}).
	\item \textbf{Gap analysis.} A quantitative comparison with serverless, microservice, and batch workloads, identifying the granularity, timescale, and predictability gaps that render existing resource controls ineffective (\S\ref{sec:gap}).
	\item \textbf{System.} \sys, an eBPF-based resource controller with hierarchical cgroup domains, in-kernel enforcement, and runtime-adaptive policies, with preliminary evaluation demonstrating improved isolation and reduced waste (\S\ref{sec:design}, \S\ref{sec:eval}).
\end{itemize}

\section{Background}

\textbf{AI Coding Agents.} Modern AI coding agents combine large language models (LLMs) with tool-use capabilities to autonomously solve software engineering tasks. These agents operate in an iterative loop: the LLM reasons about the current state, selects a tool (e.g., file read, code edit, shell command, test execution), executes the tool in a sandboxed environment, observes the result, and repeats until the task is complete. Representative agents include Claude Code~\cite{claude-code}, OpenHands~\cite{openhands}, and SWE-agent~\cite{swe-agent}. Each tool invocation spawns distinct processes with varying resource profiles; for example, a compiler may consume gigabytes of memory, while a simple file read uses minimal resources. This creates highly dynamic, phase-varying workloads that challenge traditional resource management approaches.

\textbf{Linux cgroup} provides a hierarchical resource governance abstraction where the kernel organizes tasks into a tree of control groups and applies controller-specific accounting and enforcement along that hierarchy~\cite{cgroupv2}. The memory controller exposes two key boundaries: \texttt{memory\allowbreak.high} as a soft throttle point that triggers reclaim pressure without invoking the OOM killer, and \texttt{memory\allowbreak.max} as a hard limit that triggers OOM when exceeded. Cgroup~v2 also provides lifecycle controls: \texttt{cgroup\allowbreak.freeze} stops all processes in a subtree until unfrozen, \texttt{cgroup\allowbreak.kill} terminates all processes while handling concurrent forks, and \texttt{memory.oom\allowbreak.group} ensures atomic OOM termination to avoid partial failures.

\textbf{eBPF} enables Linux to address the tension between standardized interfaces and dynamic workloads by introducing programmable enforcement points, providing a safe and dynamically loadable mechanism for executing control logic inside the kernel~\cite{ebpf-verifier}. On the CPU side, \texttt{sched\_ext} allows scheduling policies to be defined by BPF programs with fail-safe reversion to default behavior on errors~\cite{sched-ext}. On the memory side, \texttt{memcg\_bpf\_ops} introduces hooks such as \texttt{get\_high\_delay\_ms} for custom throttle delays on \texttt{memory.high} breaches~\cite{memcg-bpf}. These primitives enable in-kernel enforcement with microsecond-level reaction times.

\section{Agent Workload Characterization}
\label{sec:characterization}

We conducted the first systematic measurement study of resource usage patterns in production AI coding agents, aiming to answer the following research questions: \textbf{RQ1}: What is the execution model of agents? \textbf{RQ2}: What are the characteristics of resource usage in agent workloads? We also discuss the fundamental differences between AI coding agent resource usage patterns and traditional containerized workloads, and the impact of these differences on the applicability and efficiency of existing resource management tools.

\subsection{Experimental Setup}

\textbf{Experimental Platform.} All experiments run on a machine with an Intel Core Ultra 9 285K (24 cores, 5.8\,GHz), 128\,GB DDR5 RAM, Ubuntu 24.04 with Linux 6.15.11 (cgroup v2). Each task executes in an isolated Podman container using official SWE-rebench images (2.9--17.3\,GB each). No resource limits are applied to ensure measurements reflect unconstrained demand.

\textbf{Dataset.} We adopt a two-level dataset strategy. First, we collect 111 tasks (GLM local model) and 33 tasks (Haiku cloud model) from the SWE-rebench dataset for large-scale statistical analysis. Additionally, we select 18 representative tasks covering six categories (CLI\_Tools, DevOps\_Build, ML\_Scientific, Medical\_Bio, SQL\_Data, Web\_Network) and three difficulty levels (Easy, Medium, Hard) for category-level analysis. These tasks cover typical use cases for AI coding agents, including command-line tool fixes, build system configuration, machine learning code debugging, biomedical data processing, database query optimization, and web service fixes.

\textbf{Agent Implementation.} All tasks are executed using the same agent framework, Claude Code, paired with two different underlying models: (1)~Haiku (cloud API), where LLM inference executes in the Anthropic cloud; and (2)~GLM 4.7 flash (local GPU), where LLM inference executes via GPU on the local device. The agent framework itself (tool calling logic, sandbox environment, Node.js runtime) is identical, with the difference only in the underlying model and its inference location. These two models were chosen to observe the impact of different models (and their inference locations: remote API vs. local GPU) on container resource usage.

\textbf{Data Collection.} For each task execution, we sample CPU utilization and memory usage of each container at 1-second intervals and record the type, start time, and end time of each tool call. All tasks execute in the same sandbox environment to ensure measurement comparability.

\subsection{Execution Model}

The execution process of agents differs fundamentally from traditional containerized workloads. Unlike serverless/FaaS processing short-lived stateless requests (100ms--2s), each agent task runs for approximately 10 minutes on average (GLM average 10.8 minutes, Haiku average 5.8 minutes, median 8.1 minutes; see Fig-exec (a)) and executes stateful multi-round reasoning and tool call loops. Additionally, container startup has non-negligible fixed overhead, averaging 26.5 seconds (median 23.0 seconds), with a maximum of 97 seconds.

\textbf{Approximately 60\% time for reasoning, 40\% for tool execution, but huge task variation (0\%--86\%).} Agent execution consists of alternating LLM reasoning and tool call phases. As shown in Fig-exec (b), across all 144 tasks (Haiku 33 + GLM 111), using \texttt{active\_time} (time span from trace first to last record, excluding container startup overhead) as denominator, tool execution time median is about 35\% (Haiku 34.7\%, GLM 36.5\%), with means of Haiku 42.5\% and GLM 36.4\%. The medians of both models are close, indicating that the phase division of approximately 60\% time for LLM thinking and 40\% time for tool execution is a common feature of agent workloads. However, this ratio varies dramatically between tasks, ranging from 0\% to 86\% (median ~36\%), as shown in Fig-tool-time (a). 

\textbf{LLM reasoning accounts for only 26--44\% of end-to-end latency; the remainder is infrastructure overhead.} Analyzing end-to-end task latency using \texttt{claude\_time} (full container lifecycle) as the denominator: container startup overhead accounts for 31--48\% (Haiku 47.7\%, GLM 31.0\%), tool execution accounts for approximately 26\% (consistent across both models: Haiku 25.9\%, GLM 25.5\%), and actual LLM reasoning accounts for only 26\% (Haiku) to 44\% (GLM). In other words, infrastructure overhead (container startup + tool execution) consumes 57--74\% of end-to-end latency. This finding demonstrates that OS-level container and resource management directly affects more than half of agent task latency. Optimizing resource control during tool execution is therefore not merely a matter of resource efficiency, but directly impacts user-perceived task completion time.


\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{exec_overview}
	\caption{Task execution time distribution (a) and execution phase division (b).}
	\label{fig:exec}
\end{figure}

\textbf{Bash and Task account for over 90\% of tool execution time, but different models have drastically different tool strategies.} As shown in Fig-tool-type, analyzing tool call structure using Haiku agent (33 tasks) as an example (Fig-tool-type (a)). Task (sub-agent calls) and Bash account for 47.8\% (n=17) and 43.2\% (n=410) of total tool execution time respectively, with WebSearch/WebFetch combined about 5\%, and other tools (Read, Grep, Edit, etc.) totaling less than 5\%. In contrast, GLM agent relies almost entirely on Bash calls (accounting for 99.5\% of tool time, n=3304), without using Task sub-agents and Web search tools. Further analyzing semantic categories of Bash commands (Fig-tool-type (b)), in Haiku, test execution (pytest, unittest, etc.) accounts for 72.9\% of total Bash time, and package installation accounts for 10.8\%; GLM's distribution is more dispersed, with test execution at 43.7\%, Python snippets 26.9\%, and package installation 10.1\%. Different types of tool calls have distinctly different resource consumption characteristics: test execution is typically CPU and memory intensive, while file exploration and Git operations are relatively lightweight.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{tool_bash_breakdown}
	\caption{Tool execution time distribution (a) and Bash command semantic category proportion (b), GLM agent.}
	\label{fig:tool_type}
\end{figure}

\textbf{Execution times of different tool types span three orders of magnitude.} Haiku's Task sub-agent has an average execution time as high as 100.47 seconds (n=17), Bash commands average 3.76 seconds (n=410); GLM's Bash commands average 5.93 seconds (n=3304). In contrast, Read (Haiku avg 0.34s / GLM avg 0.08s) and Edit (Haiku avg 0.05s / GLM avg 0.04s) operations are three orders of magnitude faster. This difference indicates that different tool types require different resource allocation strategies. The difference in tool strategies between the two models also has resource management implications: Haiku model distributes some work to external services through Task sub-agents and Web search, while GLM model concentrates all computation on local Bash execution, resulting in higher local resource consumption (Bash total time 19598s vs. Haiku 1543s).

\textbf{Tool calls follow ``understand-modify-verify'' temporal pattern.} As shown in Fig-tool-time (b), analyzing tool call distribution by dividing the execution process into 10 equal-length phases, we found Read operations concentrate in early execution (first 30\%), corresponding to the code comprehension phase; Bash calls are most dense in middle-to-late phases (40--80\%), corresponding to testing and verification phases; Edit operations are relatively evenly distributed throughout the execution process. This phased characteristic aligns with the software engineering ``understand-modify-verify'' workflow, providing a basis for phase-aware resource control.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{tool_time_pattern}
	\caption{Tool time proportion distribution (a) and tool call distribution over execution progress (b), all 144 tasks.}
	\label{fig:tool_time}
\end{figure}

\subsection{Resource Usage Characteristics}

Building on the execution model described in RQ1, this section further analyzes the resource usage characteristics of agent workloads. We first present a macro portrait of resource occupation, then analyze their unpredictability from three dimensions: temporal dynamics, non-determinism, and heterogeneity.

\textbf{Memory, not CPU, is the primary bottleneck for multi-tenant concurrency density.} Agent average CPU utilization is low (Haiku 13.2\%, GLM 7.6\%, single-core baseline), well below saturation on the 24-core experimental platform. Memory is the binding constraint for concurrency density: peak memory can reach 2--4\,GB, so 128\,GB memory with peak allocation can only support 32--64 instances, while CPU utilization at this concurrency still does not exceed 36\% of total capacity. This imbalance means dynamic memory management is key to improving concurrency density: elastic expansion during brief memory bursts, and resource reclamation during idle periods to accommodate more concurrent instances.

\textbf{Disk storage requirements far exceed traditional containerized applications.} As shown in Fig-resource (a), among 114 deduplicated independent images, Docker image sizes range from 2.9GB to 17.3GB, averaging 4.1GB with median 3.5GB, and highly concentrated in the 3--4GB range, with total image volume for GLM 111 tasks alone reaching 456GB, creating significant storage pressure for multi-tenant deployment.

\textbf{Resource consumption exhibits a two-layer structure: a ${\sim}$185\,MB framework baseline plus tool-call bursts.} As shown in Fig-resource (b), the agent framework (e.g., Claude Code's Node.js runtime) maintains an irreducible memory baseline: across all 144 tasks, average memory in early execution (first 10\% of sampling points) is Haiku 183\,MB, GLM 188\,MB, median about 185\,MB. Even during LLM reasoning phases (no tool calls), memory remains at this baseline level. Resource fluctuations come almost entirely from subprocesses spawned by tool calls: test execution, dependency installation, and other operations raise memory to 500\,MB--2\,GB, then fall back to baseline. After normalizing memory usage of all tasks to execution progress and aggregating (Fig-resource (b)), the first half of execution (0--50\%) maintains a relatively stable baseline (about 185--200\,MB), while the second half shows an upward trend with greater variance, consistent with resource consumption during Bash-dense phases. In multi-tenant scenarios, 64 concurrent instances need about 12\,GB of memory for framework baseline alone, while tool bursts are the main resource management challenge. Resource control strategies need to distinguish these two layers: provide stable guarantees for the framework baseline, and elastic scheduling for tool bursts.

\textbf{Resource consumption determined by tool semantics rather than tool type, same Bash calls can differ by 13.7 times.} Same type of tool calls (e.g., Bash) produce vastly different resource consumption on different tasks. For example, among 18 representative tasks, Bash calls in Medical\_Bio category consume average peak memory of 4GB, while Web\_Network category needs only 291MB, difference reaching 13.7 times. This indicates resource requirements are not determined by tool type, but by specific semantics of tool execution: test execution, data loading, and model inference all have completely different resource characteristics even when invoked through the Bash tool. This finding has important implications for resource control strategies: resource allocation based solely on tool type is insufficient, requiring finer-grained semantic-aware control. Further quantifying resource burst amplitude of different Bash command categories: test execution (pytest, etc.) P95 memory spike up to 518MB (Haiku)/234MB (GLM), average CPU spike +3.2\%; package installation P95 memory spike about 233MB; file exploration and Git operations average memory spike only 4.5MB and 13.5MB. Same Bash calls, memory burst amplitude of different semantic categories differs by two orders of magnitude (518MB vs. 4.5MB), providing direct basis for differentiated resource quotas based on command semantics.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{resource_profile}
	\caption{Docker image size distribution (a) and aggregated memory trajectory (b), all 144 tasks.}
	\label{fig:resource}
\end{figure}

This unpredictability of resource characteristics stems from three dimensions: temporal dynamics, non-determinism, and heterogeneity.

\subsubsection{Temporal Dynamics}

Agent workloads exhibit dramatic temporal fluctuation in resource usage. It can be observed that memory usage changes by as much as 2.9GB within a single sampling interval (1 second), CPU utilization shows sharp fluctuations with peaks exceeding 100\% (multi-core utilization). Resource usage shows a clear ``burst'' pattern rather than smooth changes.

Fig-timeseries-haiku and Fig-timeseries-glm show resource usage time series of two specific tasks as examples: above is pre-commit/pre-commit\#2524 task executed with Haiku agent, below is sigmavirus24/github3.py\#673 task executed with GLM agent. Each figure contains two subplots: above is CPU utilization (blue), below is memory usage (green), red shaded intervals mark execution periods of tool calls (Bash, Read, Edit, etc.). It can be clearly seen that resource spikes are highly aligned with tool calls: CPU and memory peaks almost all occur during tool execution (such as running tests, installing dependencies), while resource usage during LLM reasoning phases between tool calls is relatively stable. These two examples also reflect typical resource differences of different models: Haiku task tool calls are relatively concentrated, each execution accompanied by obvious CPU multi-core spikes (peaks exceeding 175\%); GLM task tool calls are more dense and frequent, but CPU remains at low level (~10\%) long-term, resource fluctuations mainly reflected in memory dimension. Although specific resource curves vary by task, this ``burst-silence'' alternating pattern driven by tool calls is universally present across all tasks in both datasets.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{rq1_resource_timeseries}
	\caption{Resource usage time series: Haiku agent executing pre-commit/pre-commit\#2524.}
	\label{fig:timeseries_haiku}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{rq1_resource_timeseries_qwen}
	\caption{Resource usage time series: GLM agent executing sigmavirus24/github3.py\#673.}
	\label{fig:timeseries_glm}
\end{figure}

\textbf{98.5\% of memory bursts occur during tool calls, resource fluctuations almost entirely driven by tool execution.} To quantify this phenomenon, we label each resource sampling point as ``during tool call'' or ``during LLM reasoning'', and count memory burst events exceeding 300MB for attribution. In Haiku dataset, tool calls account for only 50.6\% of total sampling time, yet contain 98.5\% of memory bursts; in GLM dataset, tool calls account for 35.9\% of time, containing 67.3\% of memory bursts. Concentration of memory bursts in both datasets is 1.9$\times$, meaning the probability of memory bursts during tool calls is nearly 2 times their time proportion. Attribution of CPU bursts is more dispersed: 55.3\% of CPU bursts in Haiku occur during tool calls, only 30.2\% in GLM (GLM's CPU bursts mainly come from local GPU inference scheduling overhead). This asymmetry (memory bursts highly concentrated in tool calls, CPU bursts more evenly distributed) means memory resources should be elastically scheduled at tool call granularity, while CPU resources need broader context awareness.

As shown in Fig-change-rate, we observed maximum memory change rate reaching 3GB/second, maximum CPU change rate exceeding 50\%/second. Significant change events account for 1.7\%--3.8\% of all sampling points.

\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{rq1_change_rate_distribution}
	\caption{Resource change rate distribution (CPU and memory), Haiku dataset.}
	\label{fig:change_rate}
\end{figure}

\textbf{Resource bursts last 1--2 seconds with peak-to-average ratio up to 15.4$\times$, making static limits ineffective.} Resource bursts are short-lived (spikes shown in Fig-timeseries-haiku and Fig-timeseries-glm). Among 18 representative tasks, the Medical\_Bio\_Hard task has peak memory of 4060\,MB while average memory is only 264\,MB, an overprovisioning factor of 15.4$\times$. This 4\,GB peak lasts only about 1--2 seconds before falling back to the 230\,MB baseline. This spike pattern means that static limits set to peak waste resources 98\% of the time, while limits set to average trigger OOM kills during 1--2 second tool bursts. Traditional reactive resource adjustment cannot cope with such short-lived bursts.

\textbf{CPU-memory correlation varies by task ($-$0.84 to $+$0.50); co-directional change cannot be assumed.} As shown in Fig-timeseries-haiku and Fig-timeseries-glm, correlation between CPU and memory usage shows significant task dependency. Average correlation coefficient across all tasks is -0.39 (ranging from -0.84 to +0.50), indicating CPU peaks and memory peaks do not appear synchronously in most tasks. Some tasks show positive correlation (tool execution phases pull up both CPU and memory), others show negative correlation (CPU-intensive phases have lower memory demand, and vice versa). This task-dependent coupling pattern is harder to cope with than simple positive/negative correlation: resource control strategies cannot assume CPU and memory demands always change in same direction, must independently monitor and coordinate management of both resource dimensions.

Memory peaks concentrate in the latter half of execution (mean/median about 65\% progress, see Fig-resource~(b)), but the distribution covers the entire execution cycle, further increasing the difficulty of static resource allocation.

\subsubsection{Non-determinism}

Unlike traditional containerized workloads, agent workloads exhibit high non-determinism. Even executing the same task multiple times, resource usage patterns and execution results differ significantly. We executed the same task (DevOps\_Build\_Hard) three times, observing execution times of 402, 222, and 259 seconds respectively, a 1.8$\times$ difference. The three executions produced completely different solutions, including different code modifications, different numbers of file changes, and different implementation strategies. This non-determinism stems from the randomness of LLM reasoning and diversity of agent decision paths, making it difficult to predict resource requirements from historical data.

\textbf{85\%--97\% of tasks contain retry loops, consuming 7\%--20\% of execution time and causing progressive memory accumulation.} Retry is inherent behavior of agent workloads: 85\% (28/33) of tasks in Haiku dataset contain retry groups (3 or more consecutive Bash calls), in GLM dataset this ratio reaches 97\% (108/111). GLM averages 3.9 retry groups per task, maximum consecutive retries up to 56 times, retry time averages 20.5\% of total execution time (Haiku 7.4\%). This ``execute test, observe failure, modify code, test again'' iterative pattern is agent's behavioral signature. More importantly, each retry cycle retains previous memory context without cleanup, causing progressive memory accumulation; in the most extreme case, retries cause 502\,MB of memory accumulation that is not released. This behavioral pattern is rare in traditional containerized workloads but common in agent workloads, and its resource impact cannot be predicted in advance. Even LLM-observable proxies offer no help: conversation rounds correlate moderately with execution time (r\,=\,+0.57 to +0.82) but not with peak memory (r\,$<$\,0.11), confirming that resource consumption is driven by tool-call semantics rather than LLM reasoning scale.

\subsubsection{Heterogeneity}

Significant differences exist in resource requirements between different tasks and different models. Among 18 representative tasks, peak memory requirements range from 197MB to 4GB, coefficient of variation (CV) reaches 147\%. ML\_Scientific and Medical\_Bio category tasks exhibit significantly higher memory requirements than CLI\_Tools or Web\_Network tasks, but all tasks run in same container.

Haiku and GLM show significant CPU utilization difference on same tasks (Haiku average 13.2\%, GLM average 7.6\%, difference 1.7 times). Average execution times also differ significantly (Haiku 352 seconds, GLM 664 seconds). This result indicates resource requirements depend not only on task itself, but also on choice of underlying model.

\textbf{Simply changing underlying model produces completely different resource profile.} This CPU utilization difference reflects the essential issue: even with identical agent framework (both Claude Code), simply changing underlying model produces completely different resource profile. Haiku performs inference via cloud API, API calls, response parsing, and context management consume more local CPU; GLM's inference executes on local GPU, CPU load in container comes almost entirely from tool calls (GLM container has only 0.5\% of sampling points with CPU utilization exceeding 50\%, while Haiku reaches 8.2\%). Even executing the same task, CPU distribution, execution duration, and memory trajectories of both models differ significantly. This means resource management strategies cannot make static assumptions based on single category of ``agent workloads'', must adapt to resource profile differences brought by different models.

\section{Resource Management Gaps}
\label{sec:gap}

The characterization in \S\ref{sec:characterization} reveals that agent workloads differ from known cloud workloads along several dimensions (Table~\ref{tab:comparison}). These differences create three gaps in existing resource management (Table~\ref{tab:gaps}): granularity, timescale, and predictability. We analyze each gap below, explaining why solutions from kernel cgroup interfaces to cluster-level autoscaling cannot bridge them.

\begin{table*}[t]
	\centering
	\begin{tabular}{lcccc}
		\hline
		\textbf{Dimension} & \textbf{Serverless/FaaS} & \textbf{Microservices} & \textbf{Batch/HPC} & \textbf{AI Coding Agent}                    \\
		\hline
		Execution duration & 100ms--2s                & Long-running           & Minutes--hours     & \textbf{5--11 minutes}                      \\
		Statefulness       & Stateless                & External state         & Stateful           & \textbf{In-process stateful}                \\
		Memory peak/avg    & ~1.5$\times$             & 2--3$\times$           & ~1$\times$         & \textbf{15.4$\times$}                       \\
		Determinism        & Deterministic            & Mostly deterministic   & Deterministic      & \textbf{1.8$\times$ variance for same task} \\
		Resource pattern   & Flat                     & Steady + daily cycle   & Stable rise        & \textbf{Burst-silence alternating}          \\
		Termination cost   & Just retry               & Can migrate            & Lose progress      & \textbf{Lose all LLM context}               \\
		\hline
	\end{tabular}
	\caption{Quantitative comparison of AI coding agent workloads with typical cloud workloads.}
	\label{tab:comparison}
\end{table*}

\begin{table}[t]
	\centering
	\small
	\begin{tabular}{p{1.5cm}p{2.8cm}p{3.2cm}}
		\hline
		\textbf{Gap}          & \textbf{Agent Characteristic}                                           & \textbf{Why Existing Tools Fail}                                                            \\
		\hline
		Granularity           & Peak/avg 15.4$\times$; 185\,MB baseline + tool bursts                   & \texttt{memory.max/high}: single static threshold; K8s QoS: container-level policy           \\
		Timescale             & 1--2\,s bursts; $\Delta$ up to GB/s                                     & PSI/oomd: ms-level user-space loop; VPA: minute-level resize                                 \\
		Predictability        & 1.8$\times$ run variance; non-deterministic tool paths                   & VPA/Autopilot: historical P95 invalid; 20$\times$ cross-task variation                       \\
		\hline
	\end{tabular}
	\caption{Three resource management gaps and why existing mechanisms fail for agent workloads.}
	\label{tab:gaps}
\end{table}

\subsection{Granularity Gap}

Agent resource demands vary at tool-call granularity, but all existing controls set a single policy at container level. The memory peak-to-average ratio illustrates this most clearly: Azure Functions exhibit near-flat memory~\cite{shahrad2020serverless}, Azure VMs stay within 2--3$\times$~\cite{cortez2017resource}, and Google Autopilot recommends within 2$\times$ of actual peaks~\cite{rzadca2020autopilot}, whereas agent workloads reach 15.4$\times$.

Cgroup~v2 hard limits (\texttt{memory\allowbreak.max}) trigger OOM kill synchronously in the page allocation path. For traditional workloads with peak/avg ratio 2--3$\times$, setting limits to peak causes 50--67\% overprovisioning, usually acceptable. But with agent peak/avg of 15.4$\times$, setting to peak (4060\,MB) wastes 93\% of allocated memory, while setting to average triggers OOM kills during 1--2 second tool bursts, destroying all accumulated agent state. Across all 144 tasks, CPU overprovisioning by peak allocation reaches 11.1$\times$--13.9$\times$. A static threshold fundamentally cannot express the ``low baseline + high burst'' two-layer structure. Soft limits (\texttt{memory\allowbreak.high}) avoid OOM kills through reclaim and throttling, which is effective for traditional workloads where reclaim targets file page cache without affecting correctness. However, agent workloads have two structural problems: reclaim cannot distinguish framework memory (Node.js heap, V8 JIT cache, ${\sim}$185\,MB) from tool subprocess memory, and acting on framework pages causes V8 GC pressure and JIT cache invalidation that extends LLM response parsing time; additionally, a single threshold at container granularity cannot differentiate \texttt{git~status} (13.5\,MB average spike) from \texttt{pytest} (518\,MB P95 spike). Kubernetes QoS classes face the same limitation: Guaranteed allocation wastes 91--93\%, BestEffort is unsuitable for stateful agents, and Burstable with CFS bandwidth control (\texttt{cpu\allowbreak.max}) still operates at container granularity, unable to distinguish the LLM reasoning phase (near-zero CPU, low memory) from tool execution (multi-core burst, high memory) or set per-tool-call quotas. Agent CPU demand is bimodal (GLM: only 0.5\% of sampling points exceed 50\% utilization, yet tool execution peaks exceed 175\%), and a single bandwidth quota cannot adapt to both modes.

\subsection{Timescale Gap}

Agent resource bursts last only 1--2 seconds with change rates reaching several GB/s, a burst-silence pattern unlike the flat profiles of serverless functions, the diurnal cycles of microservices, or the steady ramps of batch jobs (Table~\ref{tab:comparison}). User-space controllers cannot react fast enough. PSI-driven solutions (systemd-oomd, Meta oomd) monitor cgroup Pressure Stall Information and take action (kill the cgroup or trigger swap) when pressure exceeds a threshold. Their design assumes (a) a sufficient time window for decision-making after the pressure signal appears, and (b) that kill or migration is an acceptable degradation path. Both assumptions fail for agents: the full cycle from PSI signal generation to user-space daemon reception, decision, and cgroup control file write takes tens of milliseconds, by which time a 1--2 second burst (with change rates of 3\,GB/s) has already passed or triggered kernel intervention. PSI is also a container-level aggregate that cannot attribute pressure to specific tool calls; it reports ``container has pressure'' but not ``pytest caused it.'' Kubernetes VPA adjusts resources at Pod restart level (stable) or minute-level in-place resize (alpha feature), orders of magnitude slower than second-scale tool bursts, making within-execution adjustment impossible.

\subsection{Predictability Gap}

Traditional cloud workloads are largely deterministic, allowing history-based resource management: Borg's utilization data guides allocation, and Autopilot~\cite{rzadca2020autopilot} uses historical P95 metrics for automatic recommendations. Agent workloads violate this assumption along two axes. First, resource demand varies 20$\times$ across task categories (CLI\_Tools peak memory 197\,MB vs.\ Medical\_Bio 4060\,MB), so VPA-recommended values are inevitably too high or too low. Second, even the same task produces different resource demands across runs (1.8$\times$ execution time variance), and output token count has near-zero correlation with peak memory (r\,=\,$-$0.14), making historical percentiles statistically meaningless.

This unpredictability is compounded by agents' in-process statefulness: an OOM kill destroys an average of 10 minutes of accumulated LLM context, and non-determinism means re-execution does not guarantee the same solution path. Any strategy that relies on ``kill + restart'' as a fallback, including Kubernetes eviction and systemd-oomd, is therefore prohibitively expensive for agent workloads.

In summary, control granularity is mismatched with tool-call-level dynamics, reaction timescale is mismatched with sub-second bursts, and prediction assumptions are invalidated by non-determinism. \sys is designed to close these three gaps.

\section{Design and Implementation}
\label{sec:design}

\subsection{Overview}

\sys is designed around three goals derived from the three gaps identified in \S\ref{sec:gap}:

\begin{itemize}
	\item \textbf{Fine-grained resource alignment} (granularity gap): Resource domains align with tool-call boundaries, adapting to phase transitions rather than static limits.
	\item \textbf{Microsecond-level responsiveness} (timescale gap): Control logic reacts to resource pressure at sub-millisecond timescales, before interference forms.
	\item \textbf{Runtime adaptability} (predictability gap): Policies adjust based on real-time observation rather than historical prediction, with eBPF verification~\cite{ebpf-verifier} and fail-safe reversion~\cite{sched-ext}.
\end{itemize}

\sys achieves these goals through two complementary mechanisms: a hierarchical cgroup structure that organizes resources around agent workloads and tool calls, and eBPF-based enforcement that executes control logic at kernel enforcement points.

\subsection{Dynamic Resource Domains}

\sys organizes resources using a hierarchical cgroup v2 structure. Each agent workload maps to a cgroup node, with tool calls as child nodes. This enables per-tool-call resource constraints while maintaining overall workload budgets.

For recovery, \sys uses cgroup v2 lifecycle primitives. When a tool call exceeds soft limits, \sys freezes the subtree to allow adaptation. When termination is necessary, \sys kills the subtree atomically.

\subsection{In-Kernel Enforcement via eBPF}

\sys uses eBPF to execute control logic directly at kernel enforcement points, enabling microsecond-level reaction without user-kernel round trips.

On CPU scheduling, \sys uses \texttt{sched\_ext}, which exposes a full scheduling interface to BPF programs~\cite{sched-ext}. The BPF scheduler maintains per-workload and per-tool-call metadata in BPF maps, prioritizing latency-sensitive tool calls and enforcing fairness. The scheduler automatically reverts to the fair-class scheduler on errors.

On memory, \sys uses \texttt{memcg\_bpf\_ops} hooks~\cite{memcg-bpf} to implement custom throttling delays when a cgroup breaches its soft limit (\texttt{memory.high}), with \texttt{memory.max} as the hard limit~\cite{cgroupv2}.

A user-space daemon provides APIs to manage cgroup subtrees and subscribes to events. The daemon updates shared BPF maps that parameterize policy functions. For portability, \sys uses libbpf and BPF CO-RE~\cite{libbpf}.

\section{Preliminary Evaluation}
\label{sec:eval}

Our evaluation is designed to answer three systems questions: (i) does \sys improve isolation and tail latency for multi-tenant agent workloads, (ii) does eBPF-based in-kernel enforcement provide benefits that static knobs and user-space control cannot match, and (iii) what is the overhead and operational risk of deploying such policies.

\subsection{Methodology}

We evaluate \sys on representative agent workloads drawn from common agent sandboxes: a code-oriented workload with compiler/test bursts, a data-analysis workload with working-set swings and memory-intensive operations, and a web/tool workload with mixed CPU and I/O phases. The workloads are executed in a multi-tenant configuration where multiple agent workloads share the same host and contend for CPU and memory. We instrument each workload and tool-call cgroup to collect memory events and scheduler statistics.

Baselines isolate the benefit of in-kernel enforcement: (1)~static cgroup~v2 knobs with fixed \texttt{cpu.max}, \texttt{memory.high}, and \texttt{memory\allowbreak.max}; (2)~a user-space controller similar to TMO's Senpai~\cite{tmo} that monitors pressure and rewrites cgroup files. We also ablate \sys by disabling eBPF enforcement to isolate the contribution of in-kernel execution.

Metrics include tail latency (p95/p99), workload completion rate, interference amplification under noisy neighbors, and resource utilization. We use \texttt{memory.events} to distinguish controlled throttling from hard failures~\cite{cgroupv2}.

\subsection{Isolation, Latency, and Efficiency}

We measure isolation by comparing tail latency violations across configurations under noisy-neighbor scenarios. For latency, we compare policy reaction time between in-kernel enforcement and user-space monitoring. For efficiency, we compare throughput and memory footprint against static over-provisioning.

\subsection{Robustness and Fail-Safe Evaluation}

Robustness is evaluated under adversarial scenarios: CPU spinning loops, fork storms, and memory blow-ups. We test whether containment remains scoped to the correct workload subtree and whether \texttt{cgroup.kill} handles concurrent forks correctly~\cite{cgroupv2}. We also verify that \texttt{memory.oom.group} ensures atomic OOM termination within workload boundaries~\cite{cgroupv2}.

\subsection{Overhead Analysis}

We measure eBPF verification cost, per-operation overhead on scheduling and memory paths, and BPF map memory overhead. We validate \texttt{sched\_ext}'s fail-safe reversion by injecting scheduler errors~\cite{sched-ext}.

\section{Conclusion}

We presented \sys, an eBPF-based resource controller for interactive AI agent workloads. By executing control logic at kernel enforcement points via sched\_ext and memcg\_bpf\_ops, \sys addresses the granularity gap through tool-call-aligned cgroup domains, the timescale gap through microsecond-level in-kernel reaction, and the predictability gap through runtime-adaptive policies. In-kernel enforcement via eBPF is necessary for effective resource management of agent workloads with second-scale bursts.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
